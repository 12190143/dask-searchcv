{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we walk through a simple example to test out an experimental implementation of gradient descent for fitting a logistic regression model. Our example dataset is taken from [here](http://mlcomp.org/datasets/872), and contains 270k intermediate game stats from a [dominion game server](https://dominion.isotropic.org/). The goal is to predict the game outcome from these stats.\n",
    "\n",
    "The data is separated into a train and test dataset, stored in a sparse format. Each row first contains the game outcome (either `-1` or `1`), and is followed by a number of column indices (1 indexed) mapping to the value. There doesn't seem to be a standard reader for this format, so I rolled my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 1 ../dominion_stats/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read(fp, n, p):\n",
    "    \"\"\"Read the sparse matrix format\"\"\"\n",
    "    x = np.zeros((n, p), dtype='f8')\n",
    "    y = np.zeros(n, dtype='i1')\n",
    "    with open(fp) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            chunks = line.split()\n",
    "            if chunks[0] == '1':\n",
    "                y[i] = 1\n",
    "            for c in chunks[1:]:\n",
    "                j, v = map(int, c.split(':'))\n",
    "                x[i, j - 1] = v\n",
    "    return x, y\n",
    "\n",
    "\n",
    "X_train, y_train = read('../dominion_stats/train', 193657, 596)\n",
    "X_test, y_test = read('../dominion_stats/test', 82996, 596)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Training Data: {0} samples\".format(X_train.shape[0]))\n",
    "print(\"Testing Data: {0} samples\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In memory with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison purposes, we'll first fit using scikit-learn, using the [LogisticRegression model](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression). The `C` parameter is set high to minimize the effect of regularization (which isn't yet implemented in the dask version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "%time logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the timings here are just to provide a sense of the order of magnitude. This example is *not* to compete with scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "After fitting, we can predict outcomes, and score our model on how well it fits using the test dataset. Scikit-learn has a nice standard interface for this, with the `predict` and `score` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = logreg.score(X_test, y_test)\n",
    "print(\"{0}% accurate\".format(s * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ~69% of the game outcomes in the test dataset were predicted correctly. There may be some settings for this model that could be tuned to achieve better performance, but that's not the point of this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In memory with dask-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a proof-of-concept, an api compatible version of the `LogisticRegression` model has been implemented using dask. Fitting is done using gradient descent. The goal here is not to necessarily be speedy, but rather to handle larger datasets by working out-of-core.\n",
    "\n",
    "Currently regularization isn't implemented, and only binomial problems are supported. Still, a few of the keywords avaialable to the scikit-learn version are also supported here, as well as the common `fit`, `predict`, and `score` methods.\n",
    "\n",
    "Note that the code is generic between numpy and dask, and works fine with either. We'll start with numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask_learn.linear_model import LogisticRegression\n",
    "\n",
    "dlogreg = LogisticRegression()\n",
    "\n",
    "%time dlogreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the fitting happened quicker (on this problem). Again, it should be noted that this is not the point of this example. \n",
    "\n",
    "---\n",
    "\n",
    "The interface of our `LogisticRegression` class is similar to that of the equivalent scikit-learn class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(m for m in dir(dlogreg) if not m.startswith('_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we'll run the same `predict` and `score` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlogreg.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = dlogreg.score(X_test, y_test)\n",
    "print(\"{0}% accurate\".format(ds * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy here is worse than that provided scikit-learn. I'm not sure why that is - there may be some things that could be done to improve the performance of our algorithm. Still, we're doing better than random guessing (50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Memory Dask Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll run the same model using dask arrays. Note that this is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Convert the numpy arrays to in memory dask arrays\n",
    "dX_train = da.from_array(X_train, chunks=(10000, 596))\n",
    "dy_train = da.from_array(y_train, chunks=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports for profiling\n",
    "from dask.diagnostics import Profiler, ResourceProfiler, visualize\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook(hide_banner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit, with profiling\n",
    "dlogreg = LogisticRegression()\n",
    "\n",
    "with Profiler() as prof, ResourceProfiler(0.1) as rprof:\n",
    "    dlogreg.fit(dX_train, dy_train)\n",
    "rprof.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize([prof, rprof])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above profile plot, one can see the distinct periods where dask was computing things, and the gaps in between where the next step was going determined (click on the wheel zoom to zoom in to see this). My machine has 4 real cores (8 virtual cores), and I'm seeing ~400% cpu usage throughout. So we're making good use of parallelism.\n",
    "\n",
    "The change in memory usage is negligible, which makes sense as we're only persisting the training arrays. However, the size of the training data is ~1 GB. For data of that size, having it all in memory is fine, but for more samples or features you'd potentially want to move to an out-of-core solution (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlogreg.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = dlogreg.score(X_test, y_test)\n",
    "print(\"{0}% accurate\".format(ds * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that we get the same results using either numpy or dask arrays (as expected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-core Dask Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll store the training data into an `hdf5` file, and run the same fitting code out-of-core. Even though the data fits in memory, this will hopefully be a descent test of how well this code performs out-of-core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "# Store the data into an hdf5 file, if it doesn't already exist\n",
    "if not os.path.exists('dominion.hdf5'):\n",
    "    with h5py.File('dominion.hdf5') as f:\n",
    "        f.create_dataset('/X', data=X_train, chunks=(10000, 596))\n",
    "        f.create_dataset('/y', data=y_train, chunks=(10000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the training data into out-of-core dask arrays\n",
    "f = h5py.File('dominion.hdf5')\n",
    "dX_train = da.from_array(f['X'], chunks=(10000, 596))\n",
    "dy_train = da.from_array(f['y'], chunks=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit, with profiling\n",
    "dlogreg = LogisticRegression()\n",
    "\n",
    "with Profiler() as prof, ResourceProfiler(0.2) as rprof:\n",
    "    dlogreg.fit(dX_train, dy_train)\n",
    "rprof.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize([prof, rprof])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, fitting takes longer due to the overhead of reading the array from disk every iteration. We're averaging ~130% cpu usage, so this is only slightly running in parallel. This makes sense as h5py holds the GIL, which means that all disk reads (our bottleneck) must be done in serial.\n",
    "\n",
    "What's interesting here is that the change in memory usage is only ~300 MB - roughly a third the size of our training data (the initial offset is due to the data also being in memory elsewhere, but we're not using that here). Using a different chunking might reduce this even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dlogreg.score(X_test, y_test)\n",
    "print(\"{0}% accurate\".format(ds * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the fit is the same out-of-core as it is in memory.\n",
    "\n",
    "---\n",
    "\n",
    "Finally, we'll try this same experiment, but using the synchronous scheduler. This will be a bit more conservative on memory, as it won't try and load more than one subarray at once. This shouldn't decrease our runtime significantly (might even make it faster), since h5py holds the GIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.async import get_sync\n",
    "from dask.context import set_options\n",
    "\n",
    "dlogreg = LogisticRegression()\n",
    "\n",
    "with set_options(get=get_sync), ResourceProfiler(0.2) as rprof:\n",
    "    dlogreg.fit(dX_train, dy_train)\n",
    "rprof.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rprof.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the change in memory usage decreased to only ~70 MB. It also ran slightly faster, which is probably due to not fighting with the GIL when using threading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### What worked well\n",
    "\n",
    "- The code for this model was fairly cheap to create - roughly 150 lines total. The algorithm used to fit is simple, and written using familiar numpy operations.\n",
    "- By writing in a generic style, the code is able to work transparently with either numpy or dask arrays (or even a mix of both).\n",
    "- When using dask with some on disk storage, the fit was able to be done out-of-core using minimal memory, without any changes to our code.\n",
    "\n",
    "### What could be better\n",
    "\n",
    "- Gradient descent results in more iterations, with the benefit that each iteration is fairly cheap. When running out-of-core the loading cost of reading from disk is larger than the cost of computation (I/O bound). In this case it might make more sense to use a second order method with the hope of using fewer iterations.\n",
    "- There are almost certainly ways to make our gradient descent code more performant - both in terms of speed and in terms of goodness-of-fit.\n",
    "- There are many niceties that the scikit-learn model provides that aren't yet implemented.\n",
    "\n",
    "### Questions\n",
    "\n",
    "Is this work useful? Are there things I'm doing that are wrong/could be better? If it is useful, what would be a good next step?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
